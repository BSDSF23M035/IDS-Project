{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97614891",
   "metadata": {},
   "source": [
    "### Logistic Regression for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109e28",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ef49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d70660",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759216dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature   Humidity  Wind_Speed  Cloud_Cover     Pressure     Rain\n",
      "0    23.720338  89.592641    7.335604    50.501694  1032.378759     rain\n",
      "1    27.879734  46.489704    5.952484     4.990053   992.614190  no rain\n",
      "2    25.069084  83.072843    1.371992    14.855784  1007.231620  no rain\n",
      "3    23.622080  74.367758    7.050551    67.255282   982.632013     rain\n",
      "4    20.591370  96.858822    4.643921    47.676444   980.825142  no rain\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load data here\"\"\"\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Show the first 5 rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ca5c5",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be32dbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing values per column:\n",
      "Temperature    0\n",
      "Humidity       0\n",
      "Wind_Speed     0\n",
      "Cloud_Cover    0\n",
      "Pressure       0\n",
      "Rain           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check for Missing Data\"\"\"\n",
    "# Step 3: Check for missing data\n",
    "print(\"\\n Missing values per column:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49964dd",
   "metadata": {},
   "source": [
    "### Perform Minmax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f04b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaling (data, column):\n",
    "    min = data[column].min()\n",
    "    max = data[column].max()\n",
    "    return (data[column] - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd9acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Scaled Data:\n",
      "   Temperature  Humidity  Wind_Speed  Cloud_Cover  Pressure     Rain\n",
      "0     0.548885  0.851343    0.366485     0.504954  0.748370     rain\n",
      "1     0.715305  0.235520    0.297292     0.049759  0.180070  no rain\n",
      "2     0.602850  0.758193    0.068145     0.148433  0.388977  no rain\n",
      "3     0.544954  0.633821    0.352225     0.672518  0.037409     rain\n",
      "4     0.423693  0.955157    0.231829     0.476696  0.011586  no rain\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Perform Minmax Scaling on Appropraite Columns\"\"\"\n",
    "columns_to_scale = ['Temperature', 'Humidity', 'Wind_Speed', 'Cloud_Cover', 'Pressure']\n",
    "\n",
    "# Apply scaling to each column\n",
    "for col in columns_to_scale:\n",
    "    data[col] = minmax_scaling(data, col)\n",
    "\n",
    "# Preview the scaled dataset\n",
    "print(\"\\n Scaled Data:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80871fa",
   "metadata": {},
   "source": [
    "### Perform Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b02b9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature  Humidity  Wind_Speed  Cloud_Cover  Pressure  Rain\n",
      "0     0.548885  0.851343    0.366485     0.504954  0.748370     1\n",
      "1     0.715305  0.235520    0.297292     0.049759  0.180070     0\n",
      "2     0.602850  0.758193    0.068145     0.148433  0.388977     0\n",
      "3     0.544954  0.633821    0.352225     0.672518  0.037409     1\n",
      "4     0.423693  0.955157    0.231829     0.476696  0.011586     0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encode target column which is a categorical attribute (Hint: data[col].map)\"\"\"\n",
    "# Clean and convert (Targetted column)'Rain' column to numeric\n",
    "data['Rain'] = data['Rain'].str.strip().str.lower()  # Clean strings\n",
    "data['Rain'] = data['Rain'].map({'rain': 1, 'no rain': 0})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b9faf",
   "metadata": {},
   "source": [
    "### Divide Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbcec90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split (data, ratio):\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    test_set_size = int(data.shape[0] * ratio)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94cb147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 5)\n",
      "y_train shape: (2000,)\n",
      "X_test shape: (500, 5)\n",
      "y_test shape: (500,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Understand the above function and divide data into X_train, X_test, y_train, y_test\"\"\"\n",
    "# Separate features and target\n",
    "X = data.drop('Rain', axis=1)\n",
    "y = data['Rain']\n",
    "\n",
    "# Combine them for splitting (you can join and split, or split indices separately)\n",
    "combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Use your function to split\n",
    "train_data, test_data = train_test_split(combined, ratio=0.2)\n",
    "\n",
    "# Separate X and y for training and testing\n",
    "X_train = train_data.drop('Rain', axis=1)\n",
    "y_train = train_data['Rain']\n",
    "X_test = test_data.drop('Rain', axis=1)\n",
    "y_test = test_data['Rain']\n",
    "\n",
    "# Check shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bca6c",
   "metadata": {},
   "source": [
    "### Compute the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88aaffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used by consequent functions\"\"\"\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar or numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff6c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test above function\"\"\"\n",
    "\n",
    "value = 0\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced6ddc",
   "metadata": {},
   "source": [
    "### Compute the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e9d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used by consequent functions\"\"\"\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the logistic regression cost function\n",
    "\n",
    "    Args:\n",
    "      X : ndarray of shape (m, n), training data\n",
    "      y : ndarray of shape (m,), target values (0 or 1)\n",
    "      theta : ndarray of shape (n,), parameters\n",
    "\n",
    "    Returns:\n",
    "      total_cost : scalar, the logistic regression cost\n",
    "    \"\"\"\n",
    "    m = y.shape[0]  # number of training examples\n",
    "\n",
    "    h = sigmoid(np.dot(X, theta))  # predictions, shape (m,)\n",
    "\n",
    "    # To avoid log(0), clip values between a small epsilon and 1-epsilon\n",
    "    epsilon = 1e-15\n",
    "    h = np.clip(h, epsilon, 1 - epsilon)\n",
    "\n",
    "    cost = -(1/m) * (np.dot(y, np.log(h)) + np.dot((1 - y), np.log(1 - h)))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7c7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w: 0.693\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test above function\"\"\"\n",
    "m, n = X_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "cost = compute_cost(X_train, y_train, initial_w)\n",
    "print('Cost at initial w: {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b0621",
   "metadata": {},
   "source": [
    "### Compute Gradient of the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ee4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used by consequent function\"\"\"\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    "\n",
    "    Args:\n",
    "      X : ndarray of shape (m, n), data, m examples by n features\n",
    "      y : ndarray of shape (m,), target values (0 or 1)\n",
    "      w : ndarray of shape (n,), model parameters\n",
    "\n",
    "    Returns:\n",
    "      dj_dw : ndarray of shape (n,), the gradient of the cost w.r.t. parameters w\n",
    "    \"\"\"\n",
    "    m = y.shape[0]  # number of training examples\n",
    "\n",
    "    h = sigmoid(np.dot(X, w))  # predictions vector (m,)\n",
    "\n",
    "    error = h - y  # difference between predictions and true labels (m,)\n",
    "\n",
    "    dj_dw = (1 / m) * np.dot(X.T, error)  # gradient vector (n,)\n",
    "\n",
    "    return dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6a133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw at initial w: [0.379, 0.2139842990303756, 0.1500659903528615, 0.18687497410505072, 0.15560818845021596, 0.18585659942461802]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test above function\"\"\"\n",
    "#Compute and display gradient with w and b initialized to zeros\n",
    "# Suppose X_train shape is (m, n)\n",
    "m, n = X_train.shape\n",
    "\n",
    "# Add bias term (intercept) as a column of ones\n",
    "X_train_bias = np.hstack((np.ones((m, 1)), X_train))  # shape (m, n+1)\n",
    "\n",
    "# Initialize weights including bias term\n",
    "initial_w = np.zeros(n + 1)\n",
    "\n",
    "# Compute gradient\n",
    "dj_dw = compute_gradient(X_train_bias, y_train, initial_w)\n",
    "\n",
    "print(f'dj_dw at initial w: {dj_dw.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c401478",
   "metadata": {},
   "source": [
    "### Calcualte Weights Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e850309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha.\n",
    "    \n",
    "    Args:\n",
    "      X : ndarray of shape (m, n), training data\n",
    "      y : ndarray of shape (m,), target values\n",
    "      w_in : ndarray of shape (n,), initial parameter values\n",
    "      cost_function : function to compute cost\n",
    "      gradient_function : function to compute gradient\n",
    "      alpha : float, learning rate\n",
    "      num_iters : int, number of iterations\n",
    "      \n",
    "    Returns:\n",
    "      w : ndarray of shape (n,), updated parameters after gradient descent\n",
    "    \"\"\"\n",
    "    w = w_in.copy()  # make a copy to avoid modifying original\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Compute the gradient\n",
    "        grad = gradient_function(X, y, w)\n",
    "        \n",
    "        # Update the parameters\n",
    "        w -= alpha * grad\n",
    "        \n",
    "        # Optional: print cost every, say, 100 iterations for monitoring\n",
    "        if i % 100 == 0 or i == num_iters - 1:\n",
    "            cost = cost_function(X, y, w)\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310c1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 0.6901752495495662\n",
      "Iteration 100: Cost 0.5110277540264746\n",
      "Iteration 200: Cost 0.4455496818611689\n",
      "Iteration 300: Cost 0.4170230724045591\n",
      "Iteration 400: Cost 0.40246406943067087\n",
      "Iteration 500: Cost 0.3939083358653019\n",
      "Iteration 600: Cost 0.3881885803234998\n",
      "Iteration 700: Cost 0.3839073036541293\n",
      "Iteration 800: Cost 0.3803991741939883\n",
      "Iteration 900: Cost 0.3773292102265415\n",
      "Iteration 999: Cost 0.37454868914707723\n",
      "Optimal weights: [-0.92132719 -0.67258484 -0.07509013 -0.4094255  -0.11624506 -0.41273324]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Call above function on your data with appropraite parameters and fetch the optimal weights.\"\"\"\n",
    "m, n = X_train.shape\n",
    "X_train_bias = np.hstack((np.ones((m, 1)), X_train))  # add bias column\n",
    "initial_w = np.zeros(n + 1)  # including bias weight\n",
    "alpha = 0.01       # learning rate\n",
    "num_iters = 1000   # number of gradient descent steps\n",
    "optimal_w = gradient_descent(X_train_bias, y_train, initial_w, compute_cost, compute_gradient, alpha, num_iters)\n",
    "print(\"Optimal weights:\", optimal_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334d75b",
   "metadata": {},
   "source": [
    "### Calculate Predictions on Test Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cd4e537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters w.\n",
    "    \n",
    "    Args:\n",
    "      X : ndarray of shape (m, n), test data (including bias term if used)\n",
    "      w : ndarray of shape (n,), model parameters\n",
    "    \n",
    "    Returns:\n",
    "      p : ndarray of shape (m,), predictions (0 or 1)\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, w))\n",
    "    p = (probabilities >= 0.5).astype(int)  # threshold at 0.5\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab685df",
   "metadata": {},
   "source": [
    "### Calculate Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "76b528da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build a logic to estimate the model's accuracy\"\"\"\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of predictions.\n",
    "\n",
    "    Args:\n",
    "      y_true : ndarray of shape (m,), true labels (0 or 1)\n",
    "      y_pred : ndarray of shape (m,), predicted labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "      acc : float, accuracy as a percentage (0-100)\n",
    "    \"\"\"\n",
    "    correct = (y_true == y_pred).sum()\n",
    "    total = y_true.shape[0]\n",
    "    acc = (correct / total) * 100\n",
    "    return acc\n",
    "# Assuming you have test data X_test and true labels y_test\n",
    "# Don't forget to add bias term to X_test if needed\n",
    "m_test = X_test.shape[0]\n",
    "X_test_bias = np.hstack((np.ones((m_test, 1)), X_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(X_test_bias, optimal_w)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Model accuracy: {acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
